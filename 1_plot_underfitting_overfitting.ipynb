{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "============================\n",
    "Underfitting vs. Overfitting\n",
    "============================\n",
    "\n",
    "![overfitimages](https://raw.githubusercontent.com/alexeygrigorev/wiki-figures/master/ufrt/kddm/overfitting-logreg-ex.png)\n",
    "This example demonstrates the problems of underfitting and overfitting and\n",
    "how we can use linear regression with polynomial features to approximate\n",
    "nonlinear functions. The plot shows the function that we want to approximate,\n",
    "which is a part of the cosine function. In addition, the samples from the\n",
    "real function and the approximations of different models are displayed. The\n",
    "models have polynomial features of different degrees. We can see that a\n",
    "linear function (polynomial with degree 1) is not sufficient to fit the\n",
    "training samples. This is called **underfitting**. A polynomial of degree 4\n",
    "approximates the true function almost perfectly. However, for higher degrees\n",
    "the model will **overfit** the training data, i.e. it learns the noise of the\n",
    "training data.\n",
    "We evaluate quantitatively **overfitting** / **underfitting** by using\n",
    "cross-validation. We calculate the mean squared error (MSE) on the validation\n",
    "set, the higher, the less likely the model generalizes correctly from the\n",
    "training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import libraries\n",
    "\n",
    "Here we are going to import a couple of useful libraries. First let's import numpy and matplotlib. Numpy is a scientific computing library similar to Matlab, and matplotlib gives us Matlab-esque plotting capabilities.\n",
    "\n",
    "Next we will import scikit-learn libraries to construct arbitrary model pipelines, perform basic feature engineering, train/infer regression tasks, and validate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline                 # Super useful for keeping models clean\n",
    "from sklearn.preprocessing import PolynomialFeatures  # Generate more features from existing data\n",
    "from sklearn.linear_model import LinearRegression     # Simplest Regression model\n",
    "from sklearn.model_selection import cross_val_score   # Tells us how well our model performed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Generate some data\n",
    "Just a simple cosine function with noise added to it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "true_fun = lambda X: np.cos(1.5 * np.pi * X)\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.title(\"Noisy cosine function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Run the experiment\n",
    "Let's go ahead an run a simple experiment to showcase underfitting and overfitting. Here we keep adding polynomial degrees to our model until it is overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "for i in range(len(degrees)):\n",
    "    \n",
    "    # Pretty Plotting boiler plate\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Understanding the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Feature engineering\n",
    "[Fun story on feature engineering](http://blog.kaggle.com/2016/02/26/genentech-cervical-cancer-screening-winners-interview-1st-place-michael-giulio/)\n",
    "\n",
    "Feature engineering is the process of adding and removing (feature selection) features given prior knowledge of the dataset itself. For example, suppose we have categorical features such as \"positive\", \"negative\", and \"neutral\": these can either be encoded as (0,1,2), (+1,-1,0), or via one-hot encoding. Depending on the task, choosing the correct feature representation can have HUGE effects on the model performance. \n",
    "\n",
    "Conversely we can remove features or, more accurately, select more important features. This process of feature selection is usually based on some statistical model and has the following benefits:\n",
    "* simplification of models to make them easier to interpret by researchers/users,[1]\n",
    "* shorter training times,\n",
    "* avoids the [curse of dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality),\n",
    "* enhanced generalization by reducing overfitting[2] (formally, reduction of variance[1])\n",
    "\n",
    "## What we did: Polynomial Features\n",
    "Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n",
    "\n",
    "```\n",
    " polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## So what is the difference between Data Science and Machine Learning?\n",
    "Simply put, machine learning is the process of describing mathematical models that are able to learn (e.g. linear regression with gradient descent, extreme gradient boosting trees, LASSO, and neural nets with the back propagation algorithm), whereas data science/data mining is how we use these models to solve a particular task given data sources.\n",
    "\n",
    "## Supervised Classification vs Supervised Regression\n",
    "\n",
    "Supervised models have both training data $X$ and training labels $y$. Unsupervised learning models do not have training labels $y$.\n",
    "\n",
    "Classification tasks attempt to assign binary or categorical labels (multiclass) to data. For example, {spam, ham}, {fraud, not-fraud, unsure}. On the other hand, regression tasks try to infer values on $\\mathbb{R}$. For example, predicting the true value of a home based on input features.\n",
    "\n",
    "## What we did: Ordinary Linear Least Squares Regression\n",
    "Basic formulation of linear least squares regression.\n",
    "$$min ||Ax-y||^2 + reg(x) \\quad s.t. \\quad x\\in \\mathbb{R}^n$$\n",
    "\n",
    "The regularization parameter gives us control over the sparsity of our learned model by imposing penalty conditions to the learning process. However, ordinary least squares, OLS, has no regularization.\n",
    "\n",
    "```\n",
    "linear_regression = LinearRegression()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Model Validation\n",
    "This is a critical part of any successful model pipeline, and requires us to specify both a training/testing data set and a scoring function. Picking this function is extremely important and you can find more information [here](http://scikit-learn.org/stable/modules/model_evaluation.html)\n",
    "\n",
    "## Cross Validation\n",
    "Note there is a huge fundamental problem with our current pipeline, we use all of our data for both training and testing. What happens if we get data we haven't seen before? Can we trust our score generalizes to unknown data? \n",
    "\n",
    "A better method is to run a cross validation scheme like K-Fold, Leave One Out, Shuffle and Split, etc. You can find a detailed discussion [here](http://scikit-learn.org/stable/modules/cross_validation.html#cross-validation).\n",
    "![Example of K-fold CV](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "\n",
    "## Model validation neat statistical trick\n",
    "you can verify that your model is in the noise floor if, given sufficient conditions, \n",
    "$$\\sigma < \\frac{1}{2\\sqrt{n}}$$ where $n$ is the number of training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
